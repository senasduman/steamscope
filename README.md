# SteamScope

## Project Overview

The goal of SteamScope Game Data Analysis is to develop predictive models capable of forecasting the popularity of video games. By analyzing crucial features such as release date, developer information, pricing, and minimum system requirements, our aim is to create tools that can reliably estimate a game's sales volume.

## Motivation

Our project is driven by the needs of various stakeholders within the gaming industry: developers, retailers, and players alike. For developers and retailers, our predictive models offer invaluable assistance in crafting effective marketing and pricing strategies. These insights empower them to maximize their game's visibility and revenue potential. Meanwhile, for players, our models may provide actionable information on game pricing, enabling informed purchasing decisions . Through the power of machine learning, we aspire to deliver tangible benefits to all parties involved in the gaming ecosystem.

## How It Works

SteamScope Game Data Analysis leverages advanced machine learning techniques to sift through vast amounts of data from the Steam platform. By identifying patterns and correlations between key game features and sales volume, our models can make accurate predictions about a game's popularity. Developers, retailers, and players can then utilize these predictions to inform their decisions and optimize their respective strategies.

The insights generated by SteamScope have the potential to revolutionize the gaming industry. Developers and retailers can gain a competitive edge by fine-tuning their marketing campaigns and pricing structures based on our predictions. Players, on the other hand, can make more informed purchasing choices, ensuring they get the most value out of their gaming experience. Ultimately, SteamScope aims to drive growth and innovation within the gaming ecosystem, benefiting all stakeholders involved.

## Team Members

- [Fatmatüzzehra Öztürk](https://github.com/zehraozturkk)
- [Sadık Kaçakçı](https://github.com/sadikkacakci)
- [Sena Sarıduman](https://github.com/senasduman)

## Project Structure

The project is structured as follows:

- **data**: Contains datasets obtained from the Kaggle platform and scraped data.  
- **notebooks**: Jupyter notebooks/GoogleColab containing data exploration, preprocessing, model training, and evaluation.
- **models**: Saved machine learning models.
- **analysis**: Utility scripts for data preprocessing, model evaluation, etc.
- **docs**: Documentation files.

## Dependencies

- Python 3.x
- Jupyter Notebook
- NumPy
- Pandas
- PyCaret (for models)
- Matplotlib


## Data

The raw data for SteamScope Game Data Analysis was primarily sourced from a dataset available on Kaggle (https://www.kaggle.com/datasets/nikatomashvili/steam-games-dataset), which provided comprehensive information about various video games on the Steam platform. This dataset served as the foundation for our analysis, offering valuable insights into game attributes such as title, release date, developer, pricing, and minimum system requirements.

In addition to the Kaggle dataset, we enriched our data by scraping supplementary information from gaming websites and forums using tools such as Scrapy and BeautifulSoup. This allowed us to augment our dataset with additional features, including user reviews, playtime statistics, and community engagement metrics.

The combined dataset used in this project provides a holistic view of video game attributes and user interactions on the Steam platform, enabling comprehensive analysis and predictive modeling.

Due to the terms of use, we are unable to provide the raw data directly in this repository. However, detailed instructions for obtaining the data and preprocessing it for use in the project are provided in the respective notebooks. This ensures transparency and reproducibility while maintaining compliance with data usage policies.


## Function Summaries


## Preprocessing Stage-1 

The `Preprocessor` class is designed to clean and preprocess a dataset, particularly one related to game releases. This class provides several methods to handle different preprocessing tasks such as handling release dates, cleaning price columns, parsing minimum system requirements, and fixing storage and memory values.

==Table of Contents==
1. Installation
2. Usage
3. Class and Methods 
    - Preprocessor 
        - init
        - set_date
        - clean_price_column
        - parse_minimum requirements
        - fix_storage
        - fix_memory 
        - run_preprocessor 

==Installation==

To use the `Preprocessor` class, you need to have the following Python packages installed:
    - numpy
    - Python 3.6 or higher
    - pandas 
    You can install these packages using pip: 

    ```bash 
        pip install numpy pandas 
    ```

==Usage==

To use the `Preprocessor` class, import it and create an instance by passing the file path of your dataset. Then, you can call the methods provided to preprocess your data. 

```python 
    from preprocessor import Preprocessor

    # Create a Preprocessor object
    preprocessor = Preprocessor('path/to/your/file.csv')

    # Run the preprocessing steps
    cleaned_data = preprocessor.run_preprocessor()

```

==Class and Methods==

Preprocessor 

## init

```python
    def __init__(self, file_path):
```
**Description** İnitializes the `Preprocessor` object and loads the dataset from the specified file path. 

**Parameters:**
    - `file_path` (str): The path to the CSV file containing the dataset. 

## set_date

```python
    def set_date(self, fill_na):

```
**Description:** Processes the "Release Date" column to handle various data formats and invalid entries. 

**Parameters:** 
    - `fill_na` (bool) : If `True`, replaces invalid dates with `NaN`. If `False`, drops rows with invalid dates. 


## clean_price_column 

```python
    def clean_price_column(self, column_name, fill_free= True, convert_column_type_to_float=True):
```
**Description:** Cleans and processes a price column by removing currency symbols, handling "Free" values, and converting the column to float type. 

**Parameters:** 
    - `column_name` (str): The name of the price column to clean. 
    - `fill_free` (bool): If `True`, replaces "Free" with "0".
    - `convert_column_type_to_float` (bool): If `True`, converts the column to float type. 

## parse_minimum_requirements

```python
    def parse_minimum_requirements(self, keywords):
```

**Description:** Parses the "Minimum Requirements" column to extract specific keywords and their values. 

**Parameters:** 
    - `keywords` (list): A list of keywords to search for in the "Minimum Requirements" column.

**Returns:** 
    - A list of dictionaries containing the extracted requirements for each entry.

## fix_storage

```python
    def fix_storage(self):

```
**Description:** Cleans and standardizes the "Storage" column by converting storage values to MB and removing invalid entries.

## fix_memory

```python
    def fix_memory(self):

```
**Description:** Cleans and standardizes the "Memory" column by converting memory values to MB and removing invalid entries.

## run_preprocessor

```python
    def run_preprocessor(self):
```
**Description:** Runs all preprocessing steps in sequence, including handling release dates, cleaning price columns, parsing minimum requirements, and fixing storage and memory values.

**Returns:** 
    - The cleaned and preprocessed DataFrame. 

This provides a comprehensive overview of the `preprocessor.py` script, explaining the purpose and functionality of each method in the `Preprocessor` class. It should be useful for anyone looking to understand and use this script for their data preprocessing tasks.

## Preprocessing Stage-2

==Table of Contents==
1. Installation
2. Usage
3. Class and Methods 
    - Preprocessor 
        - init
        - get_common_game_types
        - parse_game_features
        - get_all_tag_frequencies
        - parse_supported_tags
        - get_common_languages
        - parse_supported_languages
        - drop_nan_review_text
        - add_is_weekend
        - split_year_month_day
        - add_seasons 
        - clean_reviews_tags
        - find_invalid_indices
        - clean_original_price
        - convert_storage_and_memory_to_gb
        - check_outliers
        - run_preprocessor
        

==Installation==

To use the `Preprocessor2` class, you need to have the following Python packages installed:
    - numpy
    - pandas 
    - Python 3.6 or higher
    You can install these packages using pip: 

    ```sh 
        pip install numpy pandas 
    ```

==Usage==

To use the `Preprocessor2` class, import it and create an instance by passing the file path of your dataset. Then, you can call the methods provided to preprocess your data. 

```python 
    from preprocessor2 import Preprocessor2

    # Create a Preprocessor object
    preprocessor = Preprocessor2('path_to_your_file.csv')

    # Run the preprocessing steps
    cleaned_df = preprocessor.run_preprocessor(
    game_types_count_value=5000,
    supported_tags_count_value=15000,
    common_languages_count_value=5000,
    drop_reviews=True
    )
    # The resulting cleaned and preprocessed DataFrame is now stored in `cleaned_df`.


```
==Class and Methods==

Preprocessor2 

## init 

```python
    def __init__(self, file_path):
```

**Purpose:**  
Initializes the Preprocessor2 object and loads the data from the specified CSV file into a DataFrame.

**Parameters:**
- `file_path` (str): Path to the CSV file containing the game data.

### get_common_game_types

```python
    def get_common_game_types(self, count_value):
```

**Purpose:**  
Identifies and returns a list of common game types (features) from the 'Game Features' column that appear more than a specified number of times.

**Parameters:**
- `count_value` (int): The minimum number of times a game type must appear to be considered common.

**Returns:**
- List of common game types.

### parse_game_features

```python
    def parse_game_features(self,game_types_count_value):
```


**Purpose:**  
Adds binary columns to the DataFrame for each common game type. Each column indicates whether a particular game type is present (1) or not (0).

**Parameters:**
- `game_types_count_value` (int): The minimum number of times a game type must appear to be considered common.

### get_all_tag_frequencies

```python
    def get_all_tag_frequencies(self,count_value):
```

**Purpose:**  
Identifies and returns a list of common tags from the 'Popular Tags' column that appear more than a specified number of times.

**Parameters:**
- `count_value` (int): The minimum number of times a tag must appear to be considered common.

**Returns:**
- List of common tags.

### parse_supported_tags

```python
    def parse_supported_tags(self,supported_tags_count_value):
    
```

**Purpose:**  
Adds binary columns to the DataFrame for each common tag. Each column indicates whether a particular tag is present (1) or not (0).

**Parameters:**
- `supported_tags_count_value` (int): The minimum number of times a tag must appear to be considered common.

### get_common_languages

```python
    def get_common_languages(self,count_value):
```

**Purpose:**  
Identifies and returns a list of common languages from the 'Supported Languages' column that appear more than a specified number of times.

**Parameters:**
- `count_value` (int): The minimum number of times a language must appear to be considered common.

**Returns:**
- List of common languages.

### parse_supported_languages

```python
    parse_supported_languages(self,common_languages_count_value):
```

**Purpose:**  
Adds binary columns to the DataFrame for each common language. Each column indicates whether a particular language is supported (1) or not (0).

**Parameters:**
- `common_languages_count_value` (int): The minimum number of times a language must appear to be considered common.

### drop_nan_review_text

```python
    def drop_nan_review_text(self):
```

**Purpose:**  
Removes rows from the DataFrame where the 'review_text' column contains NaN values.

### add_is_weekend

```python
    def add_is_weekend(self):
```

**Purpose:**  
Adds a binary column 'is_weekend' to the DataFrame, indicating whether the 'Release Date' falls on a weekend (1) or not (0).

### split_year_month_day

```python
    def split_year_month_day(self,column):
```

**Purpose:**  
Splits a date column into separate year, month, and day columns and removes the original date column.

**Parameters:**
- `column` (str): The name of the date column to split.

### add_seasons

```python
    def add_seasons(self):
```

**Purpose:**  
Adds binary columns to the DataFrame indicating whether the 'Release Date' falls in winter, spring, summer, or autumn.

### clean_reviews_tags

```python
    def clean_reviews_tags(self,drop_reviews = False):
```

**Purpose:**  
Cleans the 'review_text' column by removing or replacing rows that contain the word 'reviews'. If `drop_reviews` is True, rows containing 'reviews' are removed. Otherwise, they are replaced with a random sampled review text.

**Parameters:**
- `drop_reviews` (bool): Whether to drop or replace rows containing 'reviews'.

### find_invalid_indices

```python
    def find_invalid_indices(self, df, column):
```

**Purpose:**  
Finds and returns the indices of rows where the specified column cannot be converted to a float.

**Parameters:**
- `df` (DataFrame): The DataFrame to search.
- `column` (str): The name of the column to check.

**Returns:**
- List of indices with invalid values.

### clean_original_price

```python
    def clean_original_price(self):
```

**Purpose:**  
Cleans the 'original_price' column by removing non-numeric characters and converting the column to float. Rows with invalid or missing prices are removed.

### convert_storage_and_memory_to_gb

```python
    def convert_storage_and_memory_to_gb(self):
```

**Purpose:**  

This code converts memory and storage units from megabytes (MB) to gigabytes (GB) and removes the original MB columns from the dataframe.

### check_outliers

```python
    def check_outliers(self):
```

**Purpose:**  

This code removes rows from the dataframe where memory exceeds 500GB, storage exceeds 1TB, or original price is greater than 500 (with potential duplicates removed).


### run_preprocessor

```python
    def run_preprocessor(self,game_types_count_value,supported_tags_count_value,common_languages_count_value,drop_reviews):
```

**Purpose:**  
Runs a series of preprocessing steps on the DataFrame, including dropping NaN review texts, adding weekend and season indicators, parsing game features, supported tags, and languages, cleaning review texts, and cleaning the original price column. Drops irrelevant columns at the end.

**Parameters:**
- `game_types_count_value` (int): The minimum number of times a game type must appear to be considered common.
- `supported_tags_count_value` (int): The minimum number of times a tag must appear to be considered common.
- `common_languages_count_value` (int): The minimum number of times a language must appear to be considered common.
- `drop_reviews` (bool): Whether to drop or replace rows containing 'reviews' in the 'review_text' column.

**Returns:**
- Cleaned and preprocessed DataFrame.

This provides a comprehensive overview of the `preprocessor2.py` script, explaining the purpose and functionality of each method in the `Preprocessor2` class. It should be useful for anyone looking to understand and use this script for their data preprocessing tasks.


## Beautifulsoup

==Table of Contents==
1. Installation
2. Usage
3. Class and Methods 

## Installation

To get started with this project, you need to have Python installed on your system. Additionally, you need to install several Python libraries used in this project. You can do this by running the following command:

```bash
    pip install beautifulsoup4 requests pandas numpy lxml
```
## Usage

**Prepare the Input Data:**

Ensure you have a CSV file named `processed_data_that_scrapped_with_scrapy_test.csv` with a column named `Link` that contains URLs and a column named `review_text` where `NaN` values indicate rows that need to be processed.

**Run the Script:**

The script reads the input CSV file, processes the URLs, scrapes review texts and values from the web pages, and writes the results to a new CSV file named `processed_data_scrapped_with_scrapy_and_beautifulsoup.csv`.

**Output:**

The output CSV file will contain the original data along with the scraped review texts and values.

## Functions and Methods

`scrapy(df, nan_indices)`

**Purpose**

The `scrapy` function processes rows in the DataFrame where the `review_text` column has NaN values. It scrapes review texts and review values from the URLs specified in the `Link` column of these rows.

**Parameters**

1. `df`(pandas.DataFrame): The DataFrame containing the data to be processed. 
2. `nan_indices` (pandas.Index): The indices of the rows where the `review_text` column has `NaN` values. 

**Returns**

`pandas.DataFrame`: The DataFrame with updated `review_text` and `review_value` columns. 

**Detailed Explanation**

1. Reading the Data: The script starts by reading the csv file into a DataFrame. 

2. Filtering NaN Values: It filters out rows where the `review_text` is NaN and gets the indices of these rows. 

3. Scraping the Data: 
For each index in `nan_indices`: 
    - The script gets the URL from the `Link` column.
    - It makes a GET request to the URL using the `requests` library.
    - If the response status is 200 (OK), it parses the page using `BeautifulSoup`.
    - It extracts the review text and value from specific HTML elements.
    - It updates the DataFrame with the scraped data.
    - If an error occurs or the response status is not 200, it logs an error message and sets the corresponding fields to NaN.

4. Saving the Results: 
Finally, it saves the updated DataFrame to a new CSV file.

```python
    file_path = "processed_data_that_scrapped_with_scrapy_test.csv"
    df = pd.read_csv(file_path)
    new_df = df[df["review_text"].isna()]
    nan_indices = new_df.index

    # Call the function
    df_new = scrapy(df, nan_indices)

    # Check for missing values in the updated DataFrame
    print(df_new.isna().sum())

    # Save the updated DataFrame to a new CSV file
    df_new.to_csv("processed_data_scrapped_with_scrapy_and_beautifulsoup.csv", index=False)

```
**Helper Libraries**

1. BeautifulSoup: Used for parsing HTML and extracting data from web pages.
2. requests: Used for making HTTP requests to fetch web pages.
3. pandas: Used for data manipulation and analysis.
4. numpy: Used for numerical operations and handling NaN values.
5. re: Used for regular expressions to clean the scraped data.

By following this step, you should be able to understand the purpose of the script, how to set it up, and how to use it to scrape review data from the specified URLs.


## Scrapy

==Table of Contents==
1. Installation
2. Usage
3. Class and Methods 

## Installation

To get started with this project, you need to have Python installed on your system. Additionally, you need to install several Python libraries and Scrapy, which is used for web scraping. You can do this by running the following commands:

```bash
    pip install scrapy pandas numpy
```
## Usage

**Prepare the Input Data:**

Ensure you have a CSV file named `steam_data_applied_set_date_func.csv` with a column named `Link` that contains URLs.

**Run the Script:**

The provided code sets up and runs a Scrapy spider to scrape review texts and values from the URLs in the CSV file.

**Output:**

The spider will save the results to a new CSV file named `steam_data_applied_set_date_func_updated.csv`.

## Functions and Methods

`SteamspiderSpider`

**Purpose**

The `SteamspiderSpider` class is a Scrapy spider designed to scrape review texts and values from Steam store pages. It reads URLs from a CSV file, makes requests to these URLs, and extracts the desired data from the web pages.

**Parameters**

1. `name`: The name of the spider. 
2. `allowed_domains`: A list of domains that the spider is allowed to scrape.
3. `df`: A pandas DataFrame to store the input data and the scraped results. 

**Methods**

`start_requests(self)`

This method is called by Scrapy to start the spider. It:
- Reads the input CSV file into a DataFrame. 
- Initializes `review_text` and `review_value` columns with `None`. 
- Takes the first 100 rows for processing(this can be adjusted).
- Iterates over the URLs in the `Link` column, making requests to each URL. 

`parse(self, response)`

This method is the callback for each request made by `start_requests`. It: 
- Extracts the game name and review summary from the response using CSS selectors. 
- Updates the DataFrame with the scraped review text and value. 
- Saves the updated DataFrame to a new CSV file. 

`closed(self, reason)` 

This method is called when spider finishes its run. It prints a message indicating that all processing is done. 

**Example** 

```python
    from scrapy.crawler import CrawlerProcess
    from my_spider import SteamspiderSpider  # Replace 'my_spider' with the actual file name

    process = CrawlerProcess(settings={
        'FEED_FORMAT': 'json',
        'FEED_URI': 'output.json'
    })
    process.crawl(SteamspiderSpider)
    process.start()

```

**Helper Libraries**

1. Scrapy: Used for parsing HTML and extracting data from web pages.
2. pandas: Used for data manipulation and analysis.
3. numpy: Used for numerical operations and handling NaN values.

By following this step, you should be able to understand the purpose of the script, how to set it up, and how to use it to scrape review data from the specified URLs.

## Visualization and Analysis

==Table of Contents==
1. Installation
2. Usage
3. Class and Methods 

## Installation

To get started with this project, install the required Python libraries using:

```bash
    pip install pandas numpy matplotlib seaborn
```
## Usage

**Prepare the Input Data:**

Ensure you have a CSV file named `steam_data_with_correct_prices.csv` in the specified file path.

**Run the Preprocessor:**

Use the `Preprocessor2` class from `preprocessor2` to clean and preprocess the data.

**Analyze and Visualize Data:**

Perform various data analysis and visualization tasks using the cleaned data.

## Code Explanation

**Initial Setup** 

```python
    import pandas as pd
    import numpy as np
    import matplotlib as mpl
    import matplotlib.pyplot as plt
    import seaborn as sns
    import warnings
    warnings.filterwarnings("ignore")
    %matplotlib inline
```
- Import necessary libraries for data processing and visualization.
- Suppress warnings for cleaner output.
- Enable inline plotting with `%matplotlib inline`.

**Data Preprocessing**

```python
    from preprocessor2 import Preprocessor2
    file_path = "/content/drive/MyDrive/ML_lesson/Applied Machine Learning/data/steam_data_with_correct_prices.csv"
    preprocessor = Preprocessor2(file_path)
    game_types_count_value = 5000
    supported_tags_count_value = 5000
    common_languages_count_value = 5000
    drop_reviews = True
    cleaned_df = preprocessor.run_preprocessor(game_types_count_value, supported_tags_count_value, common_languages_count_value)

```

- Load the data using `Preprocessor2` and set parameters for preprocessing.
- Run the preprocessor to clean the data.

**Data Cleaning:** 

```python
    cleaned_df["original_price"] = cleaned_df["original_price"].replace(["Free to Play", "Free", "Free To Play"], 0)
    cleaned_df["original_price"] = cleaned_df["original_price"].str.strip("$USD ")
    cleaned_df["original_price"] = pd.to_numeric(cleaned_df["original_price"], errors='coerce')
    mean_value = cleaned_df["original_price"].mean()
    cleaned_df["original_price"].fillna(mean_value, inplace=True)
    cleaned_df["original_price"] = cleaned_df["original_price"].round(2)

```
- Replace "Free to Play" values with 0.
- Remove currency symbols from the price column.
- Convert price column to numeric, replacing errors with NaN.
- Fill NaN values in the price column with the mean value.
- Round the price values to 2 decimal places.

**Filtering and Converting Data** 

```python
    sample_df = cleaned_df[["Title", "original_price", "Release Date", "Memory_mb", "Storage_mb", "review_text", "review_value"]]
    sample_df = sample_df[~sample_df['review_text'].str.contains('user reviews', na=False)]
    sample_df["Release Date"] = pd.to_datetime(sample_df["Release Date"])
    sample_df["review_value"] = sample_df["review_value"].str.replace(r'[^\d]', '', regex=True).fillna(0).astype(int)

```

- Select relevant columns.
- Filter out rows with generic 'user reviews' text.
- Convert the 'Release Date' column to datetime format.
- Clean and convert 'review_value' to integers.

**Data Visualization** 

==Box Plot for Memory and Storage==

```python
    plt.figure(figsize=(10, 6))
    sns.boxplot(data=sample_df[['Memory_mb', 'Storage_mb']], palette='Set3')
    plt.title('Box Plot of Memory_mb and Storage_mb')
    plt.show()

```
- Create a box plot for "Memory_mb" and "Storage_mb" columns. 

==Pie Chart for Language Support== 

```python
    language_columns = ["does_support_english", "does_support_german", "does_support_french", "does_support_russian", "does_support_spanish - spain", "does_support_simplified chinese", "does_support_japanese", "does_support_italian", "does_support_portuguese - brazil", "does_support_korean", "does_support_traditional chinese", "does_support_polish", "does_support_portuguese - portugal"]
    language_support_df = cleaned_df[language_columns]
    language_support_df.columns = language_support_df.columns.str.replace("does_support_", "")
    language_support_totals = language_support_df.sum()

    plt.figure(figsize=(10, 8))
    plt.pie(language_support_totals, labels=language_support_totals.index, autopct='%1.1f%%', startangle=140)
    plt.title('Support for Different Languages')
    plt.show()

```
- Create a pie chart to show the distribution of games supporting different languages.

==Box Plot for Original Price== 

```python
    plt.figure(figsize=(10, 6))
    sns.boxplot(data=sample_df['original_price'])
    plt.title('Box Plot for Original Price')
    plt.show()

```
- Create a box plot for the 'original_price' column. 

==Histogram for Release Date== 

```python
    plt.figure(figsize=(10, 6))
    plt.hist(sample_df['Release Date'], bins=550, edgecolor='k')
    plt.title('Histogram for Release Date')
    plt.show()
```

- Create a histogram to show the distribution of release dates. 

==Bar Plot for Total Games Released Per Year==

```python
    sample_df["Release Year"] = sample_df["Release Date"].dt.year
    games_per_year = sample_df.groupby("Release Year")["Title"].count().reset_index()
    games_per_year.columns = ["Release Year", "Total Games"]

    plt.figure(figsize=(12, 8))
    sns.barplot(x="Release Year", y="Total Games", data=games_per_year)
    plt.title("Total Games Released Per Year")
    plt.xticks(rotation=45)
    plt.show()

```

- Create a bar plot showing the total number of games released each year. 

==Box Plot for Review Value==

```python
    plt.figure(figsize=(10, 6))
    sns.boxplot(x='review_value', data=sample_df)
    plt.title('Box Plot for Review Value')
    plt.show()
```
- Create a box plot for the 'review_value' column.

==Correlation Heatmap==

```python
    plt.figure(figsize=(12, 8))
    sns.heatmap(sample_df.select_dtypes(include=['float64', 'int64']).corr(), annot=True, cmap='coolwarm', fmt=".2f")
    plt.title('Correlation Heatmap for all Numeric Variables')
    plt.show()
```
- Create a heatmap to show the correlation between numeric variables.

==Pie Chart for Games Released Between 2014-2024==

```python
    games_per_year_range = games_per_year[(games_per_year["Release Year"] >= 2014) & (games_per_year["Release Year"] <= 2024)]

    plt.figure(figsize=(10, 8))
    plt.pie(games_per_year_range["Total Games"], labels=games_per_year_range["Release Year"], autopct='%1.1f%%', startangle=140)
    plt.title('Distribution of Total Games Released (2014-2024)')
    plt.show()

```
- Create a pie chart to show the distribution of games released between 2014-2024.

==Bar Plot for Free and Paid Games==

```python
    sample_df['Price Category'] = sample_df['original_price'].apply(lambda x: 'Free' if x == 0 else 'Paid')

    plt.figure(figsize=(8, 6))
    sample_df['Price Category'].value_counts().plot(kind='bar', color=['skyblue', 'salmon'])
    plt.title('Distribution of Free and Paid Games')
    plt.xlabel('Price Category')
    plt.ylabel('Count')
    plt.show()

```
- Create a bar plot to show the distribution of free and paid games.

==Bar Plot for Game Counts by Review Text and Price Category==

```python
    review_game_counts = sample_df.groupby(['review_text', 'Price Category']).size().reset_index(name='Count')

    plt.figure(figsize=(10, 6))
    sns.barplot(x='review_text', y='Count', hue='Price Category', data=review_game_counts)
    plt.title('Game Counts by Review Text and Price Category')
    plt.xlabel('Review Text')
    plt.ylabel('Count')
    plt.xticks(rotation=45)
    plt.legend(title='Price Category')
    plt.show()

```
- Create a bar plot to show game counts by review text and price category.

==Bar Plot for Top 15 Games with Most Reviews==

```python
    top_15_games = sample_df.nlargest(15, 'review_value')

    plt.figure(figsize=(12, 8))
    sns.barplot(x='review_value', y='Title', data=top_15_games)
    plt.title('Top 15 Games with Most Reviews')
    plt.xlabel('Number of Reviews')
    plt.ylabel('Game Title')
    plt.show()

```
- Create a bar plot to show the top 15 games with the most reviews.

By following this step, you should be able to understand the purpose of the script, how to set it up, and how to use it to preprocess and visualize game data from Steam.

## GameDescription_NLP 

**Data Loading and Initial Setup** 

```python
    import pandas as pd
    import numpy as np
    pd.set_option('display.max_colwidth', None)
    df = pd.read_csv("/content/drive/MyDrive/ML_lesson/Applied Machine Learning/data/processed_data_scrapped_with_scrapy_and_beautifulsoup.csv")
```
- Import necessary libraries.
- Load the CSV file into a DataFrame and set pandas to display the full width of any column.

**Cleaning and Processing Text Data** 

```python
    from collections import Counter
    import matplotlib.pyplot as plt
    import nltk
    from nltk.corpus import stopwords

    # Remove rows with NaN values in the "Game Description" column
    df_cleaned = df.dropna(subset=["Game Description"])

    # Convert the "Game Description" column to a list
    game_descriptions = df_cleaned["Game Description"].tolist()

    # Combine all text and convert to lowercase
    all_text = " ".join(game_descriptions).lower()

    # Count the frequency of each word in the text
    word_counts = Counter(all_text.split())

    # Get the top 30 most common words
    top_words = word_counts.most_common(30)

```
- Remove rows with missing game descriptions.
- Combine all descriptions into a single lowercase string.
- Count the frequency of each word and get the top 30 most frequent words.

**Filtering Stop Words and Plotting** 

```python
    # Download NLTK stop words
    nltk.download('stopwords')
    stop_words = set(stopwords.words('english'))

    # Remove stop words from the list of top words
    top_words_filtered = [(word, freq) for word, freq in top_words if word not in stop_words]

    # Plot the filtered top words
    words, frequencies = zip(*top_words_filtered)
    plt.figure(figsize=(10, 6))
    plt.bar(words, frequencies, color='skyblue')
    plt.xlabel('Words')
    plt.ylabel('Frequency')
    plt.title('Filtered Most Common Words')
    plt.xticks(rotation=45)
    plt.show()

```
- Download and load English stop words.
- Filter out stop words from the list of top words.
- Plot the filtered words and their frequencies.

## Acknowledgements

We developed this project as part of the CENG 3522 Applied Machine Learning course. Throughout the course, we encountered and learned from the challenges posed by real-world data, gaining valuable insights into data preprocessing and model development. We are grateful for the guidance and feedback provided by our esteemed teacher, Barış Ethem Süzek, during the presentations we made. His expertise and support have been instrumental in shaping our approach and refining our techniques.

Additionally, we extend our appreciation to our teammates for their collaboration and the invaluable exchange of experience and knowledge throughout the project. Their contributions have enriched our learning journey and strengthened the overall quality of our work.

We would like to express our sincere gratitude to Barış Ethem Süzek for his dedication to our learning and his unwavering support throughout the duration of this project.

## Contact

For any questions or inquiries about the project, feel free to contact us at:  
[sadikkacakci90@gmail.com],
[sena.sariduman@gmail.com],
[fatmatuzzehraozturkk@gmail.com].

